{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3b1440",
   "metadata": {},
   "source": [
    "\n",
    "# CNN–TPE Brain Tumor Classification — Training/Testing Folders Version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e556a",
   "metadata": {},
   "source": [
    "\n",
    "This notebook reproduces the CNN–TPE pipeline **using a dataset organized as**:\n",
    "```\n",
    "data/brain_tumor/\n",
    "  training/{glioma,meningioma,pituitary}/...\n",
    "  testing/{glioma,meningioma,pituitary}/...\n",
    "```\n",
    "- Hyperparameter search with **TPE (Hyperopt)**\n",
    "- Three-branch CNN per Table 4\n",
    "- Augmentation ablations + statistics (Wilcoxon, Cohen's d, Friedman + Bonferroni)\n",
    "- **Nested CV runs on the *training* split only**; the **testing** split is reserved for final evaluation\n",
    "- Exports to `exports/artifacts`, `exports/tables`, `exports/figures`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857b9cb",
   "metadata": {},
   "source": [
    "\n",
    "## Exports\n",
    "- `exports/artifacts/cnn_tpe_brain_tumor.h5`\n",
    "- `exports/artifacts/best_hyperparams.json`\n",
    "- `exports/artifacts/train_history.csv`\n",
    "- `exports/figures/confusion_matrix.png`, `exports/figures/roc_curves.png`, `exports/figures/fig3_cnn_vs_cnn_tpe.png`\n",
    "- `exports/figures/ablation_barplot.png`, `exports/figures/nested_cv_distribution.png`\n",
    "- `exports/tables/per_class_metrics.csv`, `exports/tables/test_macro_metrics.csv`, `exports/tables/confusion_matrix.csv`\n",
    "- `exports/tables/cnn_vs_cnn_tpe_summary.csv`\n",
    "- `exports/tables/table5_sessions.csv` (session placeholder)\n",
    "- `exports/tables/table10_nested_cv_runs.csv`, `exports/tables/table10_nested_cv_summary.csv`\n",
    "- `exports/tables/table11_wilcoxon_cohensd.csv`\n",
    "- `exports/tables/table12_friedman.csv`, `exports/tables/table12_posthoc.csv`, `exports/tables/table12_meansd.csv`\n",
    "- Templates: `exports/tables/table13_bland_altman_TEMPLATE.csv`, `exports/tables/table14_literature_TEMPLATE.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab62a6-5734-4781-8e8e-89e972d64b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y atplotlib || true\n",
    "!pip cache purge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd0823-ed1a-49f8-b214-d4ce6b00d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda create -n brain-tumor python=3.11 -y\n",
    "#!conda activate brain-tumor\n",
    "!pip install \"tensorflow==2.19.1\" hyperopt scikit-learn matplotlib pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97110679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, json, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, confusion_matrix, roc_curve)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import stats\n",
    "\n",
    "SEED = 2025\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "EXPORT_ROOT = BASE_DIR / \"exports\"\n",
    "ARTIFACTS_DIR = EXPORT_ROOT / \"artifacts\"\n",
    "TABLES_DIR = EXPORT_ROOT / \"tables\"\n",
    "FIGS_DIR = EXPORT_ROOT / \"figures\"\n",
    "for d in [ARTIFACTS_DIR, TABLES_DIR, FIGS_DIR]: d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# OLD:\n",
    "# DATA_DIR = BASE_DIR / \"data\" / \"brain_tumor\"\n",
    "\n",
    "# NEW (points to the folder where the notebook and Training/Testing live):\n",
    "DATA_DIR = BASE_DIR\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "CHANNELS = 3\n",
    "CLASSES = [\"glioma\",\"meningioma\",\"pituitary\"]\n",
    "CLASS_TO_ID = {c:i for i,c in enumerate(CLASSES)}\n",
    "\n",
    "DEFAULT_BATCH = 16\n",
    "DEFAULT_EPOCHS = 5\n",
    "INIT_LR = 1e-2\n",
    "\n",
    "RUN_TPE = True\n",
    "FULL_PIPELINE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de3b20-4de7-4b6a-aaba-b7e61a3e9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, numpy as np\n",
    "\n",
    "def list_images_two_splits_flexible(root):\n",
    "    root = Path(root)\n",
    "\n",
    "    def find_dir(candidates):\n",
    "        for name in candidates:\n",
    "            d = root / name\n",
    "            if d.exists() and d.is_dir():\n",
    "                return d\n",
    "        return None\n",
    "\n",
    "    # Look for Training/Testing (any case). If not found at root, also look under data/brain_tumor/\n",
    "    train_dir = find_dir([\"training\",\"Training\",\"TRAINING\",\"train\",\"Train\"])\n",
    "    test_dir  = find_dir([\"testing\",\"Testing\",\"TESTING\",\"test\",\"Test\"])\n",
    "\n",
    "    if train_dir is None or test_dir is None:\n",
    "        alt_root = root / \"data\" / \"brain_tumor\"\n",
    "        train_dir = train_dir or find_dir([alt_root/\"training\", alt_root/\"Training\"])\n",
    "        test_dir  = test_dir  or find_dir([alt_root/\"testing\",  alt_root/\"Testing\"])\n",
    "\n",
    "    if train_dir is None or test_dir is None:\n",
    "        found = [p.name for p in root.iterdir() if p.is_dir()]\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected 'training'/'testing' (any case) under {root} \"\n",
    "            f\"or {root/'data'/'brain_tumor'}. Found folders: {found}\"\n",
    "        )\n",
    "\n",
    "    def scan(split_dir):\n",
    "        # accept class folder names regardless of case\n",
    "        wanted = {\"glioma\",\"meningioma\",\"pituitary\"}\n",
    "        class_dirs = {}\n",
    "        for p in split_dir.iterdir():\n",
    "            if p.is_dir() and p.name.lower() in wanted:\n",
    "                class_dirs[p.name.lower()] = p\n",
    "\n",
    "        if set(class_dirs.keys()) != wanted:\n",
    "            present = [p.name for p in split_dir.iterdir() if p.is_dir()]\n",
    "            raise FileNotFoundError(\n",
    "                f\"Expected class folders (glioma, meningioma, pituitary) in {split_dir}. \"\n",
    "                f\"Found: {present}\"\n",
    "            )\n",
    "\n",
    "        exts = {\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\"}\n",
    "        paths, labels = [], []\n",
    "        for cls in [\"glioma\",\"meningioma\",\"pituitary\"]:\n",
    "            for fn in class_dirs[cls].rglob(\"*\"):\n",
    "                if fn.suffix.lower() in exts:\n",
    "                    paths.append(str(fn))\n",
    "                    labels.append(CLASS_TO_ID[cls])\n",
    "        return np.array(paths), np.array(labels)\n",
    "\n",
    "    train_paths, train_labels = scan(train_dir)\n",
    "    test_paths,  test_labels  = scan(test_dir)\n",
    "\n",
    "    print(f\"Using train dir: {train_dir}\")\n",
    "    print(f\"Using test  dir: {test_dir}\")\n",
    "    print(f\"Train images: {len(train_paths)} | Test images: {len(test_paths)}\")\n",
    "    return train_paths, train_labels, test_paths, test_labels\n",
    "\n",
    "# >>> Call this (replaces the old call) <<<\n",
    "train_paths, train_labels, test_paths, test_labels = list_images_two_splits_flexible(DATA_DIR)\n",
    "\n",
    "# keep the internal validation split the same:\n",
    "from sklearn.model_selection import train_test_split\n",
    "tr_paths, val_paths, tr_labels, val_labels = train_test_split(\n",
    "    train_paths, train_labels, test_size=0.15, random_state=SEED, stratify=train_labels)\n",
    "print(f\"Train2: {len(tr_paths)} | Val2: {len(val_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2037a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def path_label_generator(paths, labels, batch_size, aug, shuffle=True):\n",
    "    n = len(paths)\n",
    "    idxs = np.arange(n)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start+batch_size, n)\n",
    "            sel = idxs[start:end]\n",
    "            imgs = []\n",
    "            for p in paths[sel]:\n",
    "                img = tf.keras.preprocessing.image.load_img(p, target_size=IMG_SIZE, color_mode=\"rgb\")\n",
    "                img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                imgs.append(img)\n",
    "            X = np.stack(imgs,0)\n",
    "            y = tf.keras.utils.to_categorical(labels[sel], num_classes=len(CLASSES))\n",
    "            if getattr(aug, \"zca_whitening\", False) and not hasattr(aug, \"_fitted_zca\"):\n",
    "                tmp = X.copy()/255.0\n",
    "                aug.fit(tmp)\n",
    "                aug._fitted_zca = True\n",
    "            gen = aug.flow(X, y, batch_size=batch_size, shuffle=False)\n",
    "            Xb, yb = next(gen)\n",
    "            yield Xb, yb\n",
    "\n",
    "#def make_aug(variant: str):\n",
    "#    base = dict(rescale=1./255)\n",
    "#    if variant == \"none\":\n",
    "#        return ImageDataGenerator(**base)\n",
    "#    if variant == \"rotation\":\n",
    "#        return ImageDataGenerator(**base, rotation_range=15)\n",
    "#    if variant == \"brightness\":\n",
    "#        return ImageDataGenerator(**base, brightness_range=(0.9,1.1))\n",
    "#    if variant == \"zca\":\n",
    "#        return ImageDataGenerator(**base, zca_whitening=True)\n",
    "#    if variant == \"full\":\n",
    "#        return ImageDataGenerator(\n",
    "#            **base,\n",
    "#            rotation_range=15, shear_range=0.1,\n",
    "#            width_shift_range=0.1, height_shift_range=0.1,\n",
    "#            zoom_range=0.1, brightness_range=(0.9,1.1),\n",
    "#            horizontal_flip=True, vertical_flip=True, zca_whitening=True\n",
    "#        )\n",
    "#    raise ValueError(variant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce971d-b121-4c0f-b572-69cc71cafc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_aug(variant: str):\n",
    "    \"\"\"\n",
    "    Memory-safe augmentations.\n",
    "    'zca' uses featurewise center/std (surrogate for ZCA whitening).\n",
    "    \"\"\"\n",
    "    base = dict(rescale=1./255)\n",
    "\n",
    "    if variant == \"none\":\n",
    "        return ImageDataGenerator(**base)\n",
    "\n",
    "    if variant == \"rotation\":\n",
    "        return ImageDataGenerator(**base, rotation_range=15)\n",
    "\n",
    "    if variant == \"brightness\":\n",
    "        return ImageDataGenerator(**base, brightness_range=(0.9, 1.1))\n",
    "\n",
    "    if variant == \"zca\":\n",
    "        # ZCA surrogate: channel-wise standardization only (no huge SVD)\n",
    "        return ImageDataGenerator(\n",
    "            **base,\n",
    "            featurewise_center=True,\n",
    "            featurewise_std_normalization=True\n",
    "        )\n",
    "\n",
    "    if variant == \"full\":\n",
    "        return ImageDataGenerator(\n",
    "            **base,\n",
    "            rotation_range=15,\n",
    "            shear_range=0.1,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            brightness_range=(0.9, 1.1),\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            # NOTE: no zca_whitening here to avoid OOM\n",
    "            featurewise_center=False,              # keep False for speed in full aug\n",
    "            featurewise_std_normalization=False\n",
    "        )\n",
    "    raise ValueError(variant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5294d704-cd0c-49f7-ae77-1989664832cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_label_generator(paths, labels, batch_size, aug, shuffle=True):\n",
    "    n = len(paths)\n",
    "    idxs = np.arange(n)\n",
    "\n",
    "    # If this aug needs featurewise statistics (our ZCA surrogate), prefit once on a *small* sample\n",
    "    if getattr(aug, \"featurewise_center\", False) or getattr(aug, \"featurewise_std_normalization\", False):\n",
    "        if not hasattr(aug, \"_fitted_stats\"):\n",
    "            k = min(64, len(paths))  # small sample is fine\n",
    "            sel = np.random.choice(np.arange(n), size=k, replace=False)\n",
    "            Xs = []\n",
    "            for p in paths[sel]:\n",
    "                img = tf.keras.preprocessing.image.load_img(p, target_size=IMG_SIZE, color_mode=\"rgb\")\n",
    "                arr = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "                Xs.append(arr)\n",
    "            Xs = np.stack(Xs, 0)\n",
    "            aug.fit(Xs)                  # computes mean/std only (cheap)\n",
    "            aug._fitted_stats = True\n",
    "\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            sel = idxs[start:end]\n",
    "            imgs = []\n",
    "            for p in paths[sel]:\n",
    "                img = tf.keras.preprocessing.image.load_img(p, target_size=IMG_SIZE, color_mode=\"rgb\")\n",
    "                img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                imgs.append(img)\n",
    "            X = np.stack(imgs, 0)\n",
    "            y = tf.keras.utils.to_categorical(labels[sel], num_classes=len(CLASSES))\n",
    "            gen = aug.flow(X, y, batch_size=batch_size, shuffle=False)\n",
    "            Xb, yb = next(gen)\n",
    "            yield Xb, yb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f97c25-4f94-481a-ba3c-4911ca044e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_HYP[\"batch\"] = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b08ea-f6e8-4864-a5b6-4513999001df",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    for g in gpus: tf.config.experimental.set_memory_growth(g, True)\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d239c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_block(x, filters, kernel, pool=True, l2=0.0, act=\"relu\"):\n",
    "    x = layers.Conv2D(filters, kernel, padding=\"same\",\n",
    "                      kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x = layers.Activation(act)(x)\n",
    "    if pool:\n",
    "        x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "    return x\n",
    "\n",
    "def build_branch(x_in, spec, l2=0.0, act=\"relu\"):\n",
    "    x = x_in\n",
    "    for (f,k) in spec[\"layers\"]:\n",
    "        x = conv_block(x, f, (k,k), pool=True, l2=l2, act=act)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=act, kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    return x\n",
    "\n",
    "MENINGIOMA_SPEC = {\"layers\": [(32,3),(64,3),(128,3)]}\n",
    "GLIOMA_SPEC     = {\"layers\": [(16,3),(32,3),(64,3),(128,3)]}\n",
    "PITUITARY_SPEC  = {\"layers\": [(32,5),(64,5),(128,5)]}\n",
    "\n",
    "def build_cnn_tpe(hparams):\n",
    "    inp = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], CHANNELS))\n",
    "    m = build_branch(inp, MENINGIOMA_SPEC, l2=hparams[\"l2\"], act=hparams[\"act\"])\n",
    "    g = build_branch(inp, GLIOMA_SPEC,     l2=hparams[\"l2\"], act=hparams[\"act\"])\n",
    "    p = build_branch(inp, PITUITARY_SPEC,  l2=hparams[\"l2\"], act=hparams[\"act\"])\n",
    "    x = layers.Concatenate()([m,g,p])\n",
    "    x = layers.Dense(256, activation=hparams[\"act\"], kernel_regularizer=regularizers.l2(hparams[\"l2\"]))(x)\n",
    "    x = layers.Dropout(hparams[\"dropout\"])(x)\n",
    "    out = layers.Dense(len(CLASSES), activation=\"softmax\")(x)\n",
    "    model = models.Model(inp, out)\n",
    "    opt = optimizers.Adam(learning_rate=hparams[\"lr\"])\n",
    "    model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b4c4d-f60b-4fb3-b96a-0852697e728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- TPE search using Optuna (compatible with Python 3.12) ----\n",
    "# Install once if needed:\n",
    "# %pip install -q optuna\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "BEST_HYP = {\n",
    "    \"dropout\": 0.5, \"l2\": 1e-5, \"lr\": 1e-2, \"act\": \"relu\",\n",
    "    \"batch\": 16, \"epochs\": 5\n",
    "}\n",
    "\n",
    "def objective_optuna(trial):\n",
    "    params = {\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.2, 0.6),\n",
    "        \"l2\": trial.suggest_float(\"l2\", 1e-6, 1e-3, log=True),\n",
    "        \"lr\": trial.suggest_categorical(\"lr\", [1e-2, 1e-3, 1e-4]),\n",
    "        \"act\": trial.suggest_categorical(\"act\", [\"relu\",\"selu\",\"sigmoid\"]),\n",
    "        \"batch\": trial.suggest_categorical(\"batch\", [12,16,32]),\n",
    "        \"epochs\": trial.suggest_categorical(\"epochs\", [5,7,8,10]),\n",
    "    }\n",
    "\n",
    "    # reuse the same loaders/vars defined earlier in the notebook:\n",
    "    train_aug = make_aug(\"full\")\n",
    "    val_aug   = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    B = params[\"batch\"]\n",
    "    sp_tr = math.ceil(len(tr_paths)/B)\n",
    "    sp_va = math.ceil(len(val_paths)/B)\n",
    "    gen_tr = path_label_generator(tr_paths, tr_labels, B, train_aug, shuffle=True)\n",
    "    gen_va = path_label_generator(val_paths, val_labels, B, val_aug, shuffle=False)\n",
    "\n",
    "    model = build_cnn_tpe(params)\n",
    "    hist = model.fit(\n",
    "        gen_tr,\n",
    "        steps_per_epoch=sp_tr,\n",
    "        epochs=int(params[\"epochs\"]),\n",
    "        validation_data=gen_va,\n",
    "        validation_steps=sp_va,\n",
    "        verbose=0\n",
    "    )\n",
    "    # maximize val_accuracy\n",
    "    return float(max(hist.history[\"val_accuracy\"]))\n",
    "\n",
    "if RUN_TPE:\n",
    "    n_trials = 25 if FULL_PIPELINE else 5\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=SEED))\n",
    "    study.optimize(objective_optuna, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    bp = study.best_params\n",
    "    BEST_HYP = {\n",
    "        \"dropout\": float(bp[\"dropout\"]),\n",
    "        \"l2\": float(bp[\"l2\"]),\n",
    "        \"lr\": float(bp[\"lr\"]),\n",
    "        \"act\": str(bp[\"act\"]),\n",
    "        \"batch\": int(bp[\"batch\"]),\n",
    "        \"epochs\": int(bp[\"epochs\"]),\n",
    "    }\n",
    "    print(\"Best hyperparameters (Optuna TPE):\", BEST_HYP)\n",
    "else:\n",
    "    print(\"Using fixed hyperparameters:\", BEST_HYP)\n",
    "\n",
    "# Save best params\n",
    "from pathlib import Path\n",
    "(Path.cwd()/ 'exports' / 'artifacts').mkdir(parents=True, exist_ok=True)\n",
    "with open(Path.cwd()/ 'exports' / 'artifacts' / 'best_hyperparams.json',\"w\") as f:\n",
    "    json.dump(BEST_HYP, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BEST_HYP = {\n",
    "    \"dropout\": 0.5, \"l2\": 1e-5, \"lr\": 1e-2, \"act\": \"relu\",\n",
    "    \"batch\": 16, \"epochs\": 5\n",
    "}\n",
    "\n",
    "if RUN_TPE:\n",
    "    from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "    space = {\n",
    "        \"dropout\": hp.uniform(\"dropout\", 0.2, 0.6),\n",
    "        \"l2\": hp.loguniform(\"l2\", np.log(1e-6), np.log(1e-3)),\n",
    "        \"lr\": hp.choice(\"lr\", [1e-2, 1e-3, 1e-4]),\n",
    "        \"act\": hp.choice(\"act\", [\"relu\",\"selu\",\"sigmoid\"]),\n",
    "        \"batch\": hp.choice(\"batch\", [12,16,32]),\n",
    "        \"epochs\": hp.choice(\"epochs\", [5,7,8,10]),\n",
    "    }\n",
    "\n",
    "    train_aug = make_aug(\"full\")\n",
    "    val_aug   = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    def objective(hp_params):\n",
    "        params = dict(hp_params)\n",
    "        params[\"lr\"]     = [1e-2,1e-3,1e-4][params[\"lr\"]]\n",
    "        params[\"act\"]    = [\"relu\",\"selu\",\"sigmoid\"][params[\"act\"]]\n",
    "        params[\"batch\"]  = [12,16,32][params[\"batch\"]]\n",
    "        params[\"epochs\"] = [5,7,8,10][params[\"epochs\"]]\n",
    "\n",
    "        B = params[\"batch\"]\n",
    "        sp_tr = math.ceil(len(tr_paths)/B)\n",
    "        sp_va = math.ceil(len(val_paths)/B)\n",
    "        gen_tr = path_label_generator(tr_paths, tr_labels, B, train_aug, shuffle=True)\n",
    "        gen_va = path_label_generator(val_paths, val_labels, B, val_aug, shuffle=False)\n",
    "\n",
    "        model = build_cnn_tpe(params)\n",
    "        hist = model.fit(gen_tr, steps_per_epoch=sp_tr, epochs=int(params[\"epochs\"]),\n",
    "                         validation_data=gen_va, validation_steps=sp_va, verbose=0)\n",
    "        return {\"loss\": 1.0 - max(hist.history[\"val_accuracy\"]), \"status\": STATUS_OK}\n",
    "\n",
    "    max_evals = 25 if FULL_PIPELINE else 5\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals,\n",
    "                trials=trials, rstate=np.random.default_rng(SEED))\n",
    "\n",
    "    BEST_HYP = {\n",
    "        \"dropout\": float(best[\"dropout\"]),\n",
    "        \"l2\": float(np.exp(best[\"l2\"])),\n",
    "        \"lr\": [1e-2,1e-3,1e-4][best[\"lr\"]],\n",
    "        \"act\": [\"relu\",\"selu\",\"sigmoid\"][best[\"act\"]],\n",
    "        \"batch\": [12,16,32][best[\"batch\"]],\n",
    "        \"epochs\": [5,7,8,10][best[\"epochs\"]],\n",
    "    }\n",
    "    print(\"Best hyperparameters:\", BEST_HYP)\n",
    "else:\n",
    "    print(\"Using fixed hyperparameters:\", BEST_HYP)\n",
    "\n",
    "from pathlib import Path\n",
    "( (Path.cwd()/ 'exports' / 'artifacts') ).mkdir(parents=True, exist_ok=True)\n",
    "with open(Path.cwd()/ 'exports' / 'artifacts' / 'best_hyperparams.json',\"w\") as f:\n",
    "    json.dump(BEST_HYP, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f15f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train on training (with val), evaluate on testing\n",
    "train_aug = make_aug(\"full\")\n",
    "val_aug   = ImageDataGenerator(rescale=1./255)\n",
    "test_aug  = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "B = BEST_HYP[\"batch\"]\n",
    "gen_tr = path_label_generator(tr_paths, tr_labels, B, train_aug, shuffle=True)\n",
    "gen_va = path_label_generator(val_paths, val_labels, B, val_aug, shuffle=False)\n",
    "gen_te = path_label_generator(test_paths, test_labels, B, test_aug, shuffle=False)\n",
    "\n",
    "sp_tr = math.ceil(len(tr_paths)/B)\n",
    "sp_va = math.ceil(len(val_paths)/B)\n",
    "sp_te = math.ceil(len(test_paths)/B)\n",
    "\n",
    "model = build_cnn_tpe(BEST_HYP)\n",
    "cb = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "]\n",
    "hist = model.fit(gen_tr, steps_per_epoch=sp_tr, epochs=int(BEST_HYP[\"epochs\"]),\n",
    "                 validation_data=gen_va, validation_steps=sp_va, verbose=1)\n",
    "pd.DataFrame(hist.history).to_csv('exports/artifacts/train_history.csv', index=False)\n",
    "\n",
    "y_true = test_labels\n",
    "y_prob = model.predict(gen_te, steps=sp_te, verbose=0)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "cm   = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "\n",
    "specs=[] \n",
    "for c in range(3):\n",
    "    TP=cm[c,c]; FP=cm[:,c].sum()-TP; FN=cm[c,:].sum()-TP; TN=cm.sum()-(TP+FP+FN)\n",
    "    specs.append(TN/(TN+FP+1e-12))\n",
    "spec = float(np.mean(specs))\n",
    "\n",
    "y_true_bin = label_binarize(y_true, classes=[0,1,2])\n",
    "macro_auc = roc_auc_score(y_true_bin, y_prob, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"Accuracy\": acc, \"Sensitivity/Recall\": rec, \"Specificity\": spec,\n",
    "    \"Precision\": prec, \"F1\": f1, \"Macro-AUC\": macro_auc\n",
    "}]).to_csv('exports/tables/test_macro_metrics.csv', index=False)\n",
    "\n",
    "pd.DataFrame(cm, index=CLASSES, columns=CLASSES).to_csv('exports/tables/confusion_matrix.csv')\n",
    "\n",
    "# Plots\n",
    "plt.figure()\n",
    "for i, cls in enumerate(CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "    plt.plot(fpr, tpr, label=cls)\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(\"ROC (One-vs-Rest)\"); plt.legend()\n",
    "plt.tight_layout(); plt.savefig('exports/figures/roc_curves.png', dpi=160); plt.close()\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout(); plt.savefig('exports/figures/confusion_matrix.png', dpi=160); plt.close()\n",
    "\n",
    "model.save('exports/artifacts/cnn_tpe_brain_tumor.h5')\n",
    "print(\"Saved model and reports to 'exports/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82250067",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline vs TPE figure\n",
    "base_hyp = {\"dropout\":0.5,\"l2\":1e-5,\"lr\":1e-2,\"act\":\"relu\",\"batch\":16,\"epochs\":5}\n",
    "noaug = ImageDataGenerator(rescale=1./255)\n",
    "B2 = base_hyp[\"batch\"]\n",
    "gen_tr_b = path_label_generator(tr_paths, tr_labels, B2, noaug, shuffle=True)\n",
    "sp_tr_b = math.ceil(len(tr_paths)/B2)\n",
    "gen_te_b = path_label_generator(test_paths, test_labels, B2, noaug, shuffle=False)\n",
    "sp_te_b = math.ceil(len(test_paths)/B2)\n",
    "\n",
    "base_model = build_cnn_tpe(base_hyp)\n",
    "base_model.fit(gen_tr_b, steps_per_epoch=sp_tr_b, epochs=int(base_hyp[\"epochs\"]), verbose=0)\n",
    "base_prob = base_model.predict(gen_te_b, steps=sp_te_b, verbose=0)\n",
    "base_pred = np.argmax(base_prob, axis=1)\n",
    "\n",
    "base_acc  = accuracy_score(test_labels, base_pred)\n",
    "base_prec = precision_score(test_labels, base_pred, average=\"macro\", zero_division=0)\n",
    "base_rec  = recall_score(test_labels, base_pred, average=\"macro\", zero_division=0)\n",
    "base_f1   = f1_score(test_labels, base_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "tpe_vals = pd.read_csv('exports/tables/test_macro_metrics.csv').iloc[0]\n",
    "\n",
    "metrics = [\"Accuracy\",\"Sensitivity\",\"Specificity\",\"Precision\",\"F-Score\",\"Recall\"]\n",
    "cnn_vals = [base_acc, base_rec, None, base_prec, base_f1, base_rec]\n",
    "tpe_plot = [tpe_vals[\"Accuracy\"], tpe_vals[\"Sensitivity/Recall\"], tpe_vals[\"Specificity\"],\n",
    "            tpe_vals[\"Precision\"], tpe_vals[\"F1\"], tpe_vals[\"Sensitivity/Recall\"]]\n",
    "\n",
    "x = np.arange(len(metrics)); width=0.35\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(x-width/2, [v if v is not None else 0 for v in cnn_vals], width, label=\"CNN\")\n",
    "plt.bar(x+width/2, tpe_plot, width, label=\"CNN + TPE\")\n",
    "plt.xticks(x, metrics, rotation=15); plt.ylim(0,1.05)\n",
    "plt.ylabel(\"Score\"); plt.title(\"Performance: CNN vs CNN + TPE\"); plt.legend()\n",
    "plt.tight_layout(); plt.savefig('exports/figures/fig3_cnn_vs_cnn_tpe.png', dpi=160); plt.close()\n",
    "\n",
    "pd.DataFrame({\"Metric\":metrics,\"CNN\":cnn_vals,\"CNN+TPE\":tpe_plot}).to_csv('exports/tables/cnn_vs_cnn_tpe_summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helpers and nested CV on training set only + ablation\n",
    "def ci95(xs):\n",
    "    xs = np.asarray(xs, dtype=float); m = xs.mean(); sd = xs.std(ddof=1); se = sd/np.sqrt(len(xs))\n",
    "    return m, sd, (m-1.96*se, m+1.96*se), xs.min(), xs.max()\n",
    "\n",
    "def summarize_runs(records):\n",
    "    keys = [\"acc\",\"prec\",\"rec\",\"f1\",\"auc\"]\n",
    "    rows = {}\n",
    "    for k in keys:\n",
    "        m, sd, (lo,hi), mn, mx = ci95([r[k] for r in records])\n",
    "        rows[k] = [f\"{100*m:.1f} ± {100*sd:.1f}\", f\"[{100*lo:.1f} – {100*hi:.1f}]\",\n",
    "                   f\"{100*mn:.1f}\", f\"{100*mx:.1f}\"]\n",
    "    df = pd.DataFrame(rows, index=[\"Mean ± SD\",\"95% CI\",\"Minimum\",\"Maximum\"]).T\n",
    "    df.index = [\"Accuracy (%)\",\"Precision (%)\",\"Recall (%)\",\"F1-score (%)\",\"Macro-AUC\"]\n",
    "    return df\n",
    "\n",
    "def evaluate_once(variant, hyp, paths, labels):\n",
    "    tr_p, te_p, tr_l, te_l = train_test_split(paths, labels, test_size=0.2, random_state=SEED, stratify=labels)\n",
    "    tr_p2, va_p2, tr_l2, va_l2 = train_test_split(tr_p, tr_l, test_size=0.2, random_state=SEED, stratify=tr_l)\n",
    "\n",
    "    aug_tr = make_aug(variant)\n",
    "    aug_va = ImageDataGenerator(rescale=1./255)\n",
    "    aug_te = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    if getattr(aug_tr, \"zca_whitening\", False) and not hasattr(aug_tr, \"_fitted_zca\"):\n",
    "        tmp = []\n",
    "        for p in np.random.choice(tr_p2, size=min(256, len(tr_p2)), replace=False):\n",
    "            img = tf.keras.preprocessing.image.load_img(p, target_size=IMG_SIZE, color_mode=\"rgb\")\n",
    "            tmp.append(tf.keras.preprocessing.image.img_to_array(img)/255.0)\n",
    "        aug_tr.fit(np.stack(tmp,0)); aug_tr._fitted_zca=True\n",
    "\n",
    "    B = hyp[\"batch\"]\n",
    "    gen_tr = path_label_generator(tr_p2, tr_l2, B, aug_tr, shuffle=True)\n",
    "    gen_va = path_label_generator(va_p2, va_l2, B, aug_va, shuffle=False)\n",
    "    gen_te = path_label_generator(te_p,  te_l,  B, aug_te, shuffle=False)\n",
    "    sp_tr = math.ceil(len(tr_p2)/B); sp_va = math.ceil(len(va_p2)/B); sp_te = math.ceil(len(te_p)/B)\n",
    "\n",
    "    model = build_cnn_tpe(hyp)\n",
    "    model.fit(gen_tr, steps_per_epoch=sp_tr, epochs=int(hyp[\"epochs\"]),\n",
    "              validation_data=gen_va, validation_steps=sp_va, verbose=0)\n",
    "    y_prob = model.predict(gen_te, steps=sp_te, verbose=0)\n",
    "    y_pred = np.argmax(y_prob, axis=1); y_true = te_l\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    cm   = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "    specs=[]\n",
    "    for c in range(3):\n",
    "        TP=cm[c,c]; FP=cm[:,c].sum()-TP; FN=cm[c,:].sum()-TP; TN=cm.sum()-(TP+FP+FN)\n",
    "        specs.append(TN/(TN+FP+1e-12))\n",
    "    spec = float(np.mean(specs))\n",
    "    auc = roc_auc_score(label_binarize(y_true, classes=[0,1,2]), y_prob, average=\"macro\", multi_class=\"ovr\")\n",
    "    return {\"acc\":acc,\"prec\":prec,\"rec\":rec,\"f1\":f1,\"spec\":spec,\"auc\":auc}\n",
    "\n",
    "def nested_cv(variant, hyp, paths, labels):\n",
    "    n_splits = 5; n_repeats = 5 if FULL_PIPELINE else 1\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=SEED)\n",
    "    results=[]\n",
    "    for i,(tr,te) in enumerate(rskf.split(paths, labels),1):\n",
    "        res = evaluate_once(variant, hyp, paths[tr], labels[tr])\n",
    "        results.append(res)\n",
    "        print(f\"{variant} run {i:02d}: acc={res['acc']*100:.2f}\")\n",
    "        if not FULL_PIPELINE and i>=5: break\n",
    "    return results\n",
    "\n",
    "hyp_cv = {**BEST_HYP}\n",
    "\n",
    "full_results = nested_cv(\"full\", hyp_cv, train_paths, train_labels)\n",
    "none_results = nested_cv(\"none\", hyp_cv, train_paths, train_labels)\n",
    "\n",
    "tbl10 = summarize_runs(full_results)\n",
    "tbl10.to_csv('exports/tables/table10_nested_cv_summary.csv')\n",
    "pd.DataFrame(full_results).to_csv('exports/tables/table10_nested_cv_runs.csv', index=False)\n",
    "\n",
    "def paired_test(a_list, b_list):\n",
    "    stat, p = stats.wilcoxon(a_list, b_list, zero_method=\"wilcox\", alternative=\"two-sided\", mode=\"approx\")\n",
    "    d = (np.array(b_list)-np.array(a_list)).mean() / (np.array(b_list)-np.array(a_list)).std(ddof=1)\n",
    "    return p, d\n",
    "\n",
    "rows=[]\n",
    "for k in [\"acc\",\"prec\",\"rec\",\"f1\",\"auc\"]:\n",
    "    p,d = paired_test([r[k] for r in none_results],[r[k] for r in full_results])\n",
    "    rows.append([k.upper(), p, d])\n",
    "pd.DataFrame(rows, columns=[\"Metric\",\"Wilcoxon p\",\"Cohen d\"]).to_csv('exports/tables/table11_wilcoxon_cohensd.csv', index=False)\n",
    "\n",
    "rot_results = nested_cv(\"rotation\",  hyp_cv, train_paths, train_labels)\n",
    "bri_results = nested_cv(\"brightness\",hyp_cv, train_paths, train_labels)\n",
    "zca_results = nested_cv(\"zca\",       hyp_cv, train_paths, train_labels)\n",
    "\n",
    "def friedman_series(metric):\n",
    "    A = [r[metric] for r in none_results]\n",
    "    B = [r[metric] for r in rot_results]\n",
    "    C = [r[metric] for r in bri_results]\n",
    "    D = [r[metric] for r in zca_results]\n",
    "    E = [r[metric] for r in full_results]\n",
    "    chi2, p = stats.friedmanchisquare(A,B,C,D,E)\n",
    "    return chi2, p\n",
    "\n",
    "fr_rows=[]\n",
    "for metric in [\"acc\",\"prec\",\"rec\",\"f1\",\"auc\"]:\n",
    "    chi2,p = friedman_series(metric)\n",
    "    fr_rows.append([metric.upper(), chi2, p])\n",
    "pd.DataFrame(fr_rows, columns=[\"Metric\",\"Friedman chi2\",\"p\"]).to_csv('exports/tables/table12_friedman.csv', index=False)\n",
    "\n",
    "def mean_sd(vals): return f\"{np.mean(vals)*100:.1f} ± {np.std(vals,ddof=1)*100:.1f}\"\n",
    "posthoc=[]\n",
    "meansd=[]\n",
    "for metric in [\"acc\",\"prec\",\"rec\",\"f1\",\"auc\"]:\n",
    "    A = [r[metric] for r in none_results]\n",
    "    for label, series in [(\"Rotation Only\",rot_results),(\"Brightness Only\",bri_results),\n",
    "                          (\"ZCA Whitening\",zca_results),(\"Full Augmentation\",full_results)]:\n",
    "        B = [r[metric] for r in series]\n",
    "        stat,p = stats.wilcoxon(A,B, zero_method='wilcox', mode=\"approx\")\n",
    "        p_adj = min(1.0, p*4)\n",
    "        posthoc.append([metric.upper(), label, p_adj, \"Significant\" if p_adj<0.05 else \"NS\"])\n",
    "    meansd.append([metric.upper(),\n",
    "                   mean_sd(A), mean_sd([r[metric] for r in rot_results]),\n",
    "                   mean_sd([r[metric] for r in bri_results]), mean_sd([r[metric] for r in zca_results]),\n",
    "                   mean_sd([r[metric] for r in full_results])])\n",
    "pd.DataFrame(posthoc, columns=[\"Metric\",\"Augmentation\",\"Post-hoc p (Bonf.)\",\"Significance\"]).to_csv('exports/tables/table12_posthoc.csv', index=False)\n",
    "pd.DataFrame(meansd, columns=[\"Metric\",\"No Aug.\",\"Rotation Only\",\"Brightness Only\",\"ZCA Whitening\",\"Full Aug.\"]).to_csv('exports/tables/table12_meansd.csv', index=False)\n",
    "\n",
    "# Visualizations\n",
    "acc_means = [\n",
    "    np.mean([r[\"acc\"] for r in none_results]),\n",
    "    np.mean([r[\"acc\"] for r in rot_results]),\n",
    "    np.mean([r[\"acc\"] for r in bri_results]),\n",
    "    np.mean([r[\"acc\"] for r in zca_results]),\n",
    "    np.mean([r[\"acc\"] for r in full_results]),\n",
    "]\n",
    "acc_sds = [\n",
    "    np.std([r[\"acc\"] for r in none_results], ddof=1),\n",
    "    np.std([r[\"acc\"] for r in rot_results], ddof=1),\n",
    "    np.std([r[\"acc\"] for r in bri_results], ddof=1),\n",
    "    np.std([r[\"acc\"] for r in zca_results], ddof=1),\n",
    "    np.std([r[\"acc\"] for r in full_results], ddof=1),\n",
    "]\n",
    "labels = [\"No Aug.\",\"Rotation\",\"Brightness\",\"ZCA\",\"Full\"]\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(range(len(labels)), acc_means, yerr=acc_sds)\n",
    "plt.xticks(range(len(labels)), labels)\n",
    "plt.ylabel(\"Accuracy\"); plt.title(\"Augmentation Ablation (Accuracy)\")\n",
    "plt.tight_layout(); plt.savefig('exports/figures/ablation_barplot.png', dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "full_accs = [r[\"acc\"] for r in full_results]\n",
    "plt.violinplot(full_accs, showmeans=True)\n",
    "plt.ylabel(\"Accuracy\"); plt.title(\"Nested CV: Accuracy Distribution (Full Aug)\")\n",
    "plt.tight_layout(); plt.savefig('exports/figures/nested_cv_distribution.png', dpi=160); plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1174ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Templates for tables that require external data\n",
    "pd.DataFrame(columns=[\"Tumor Type\",\"Mean Bias (%)\",\"Lower 95% LoA (%)\",\"Upper 95% LoA (%)\",\"Standard Deviation\"]).to_csv(\n",
    "    'exports/tables/table13_bland_altman_TEMPLATE.csv', index=False)\n",
    "pd.DataFrame(columns=[\"Authors\",\"Dataset\",\"Methods\",\"Accuracy\"]).to_csv(\n",
    "    'exports/tables/table14_literature_TEMPLATE.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
